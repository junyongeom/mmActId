{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94b6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "base_directories = [\n",
    "    # put path of pantomime/primary_exp/office(and open)/1/0/normal   \n",
    "]\n",
    "\n",
    "def load_all_pkl_files(base_dirs):\n",
    "    pkl_files = {}  \n",
    "    for base_dir in base_dirs:\n",
    "        if not os.path.exists(base_dir):\n",
    "            print(f\"Warning: {base_dir} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        for user_id in os.listdir(base_dir):\n",
    "            user_path = os.path.join(base_dir, user_id)\n",
    "            if os.path.isdir(user_path): \n",
    "                if user_id not in pkl_files:\n",
    "                    pkl_files[user_id] = {}\n",
    "                \n",
    "                for gesture_id in os.listdir(user_path):\n",
    "                    gesture_path = os.path.join(user_path, gesture_id)\n",
    "                    if os.path.isdir(gesture_path):  \n",
    "                        if gesture_id not in pkl_files[user_id]:\n",
    "                            pkl_files[user_id][gesture_id] = []\n",
    "                        \n",
    "                        for filename in os.listdir(gesture_path):\n",
    "                            if filename.endswith(\".pkl\"):\n",
    "                                file_path = os.path.join(gesture_path, filename)\n",
    "                                try:\n",
    "                                    with open(file_path, \"rb\") as f:\n",
    "                                        data = pickle.load(f, encoding=\"latin1\")  \n",
    "                                        pkl_files[user_id][gesture_id].append(data)\n",
    "                                except (pickle.UnpicklingError, UnicodeDecodeError) as e:\n",
    "                                    print(f\"Error loading {file_path}: {e}\")\n",
    "    return pkl_files\n",
    "\n",
    "loaded_data = load_all_pkl_files(base_directories)\n",
    "\n",
    "for user, gestures in loaded_data.items():\n",
    "    print(f\"User {user}:\")\n",
    "    for gesture, files in gestures.items():\n",
    "        print(f\"  Gesture {gesture}: {len(files)} pkl files loaded\")\n",
    "\n",
    "# labeling\n",
    "def label_data(data):\n",
    "    labeled_data = []\n",
    "    user_label_map = {}  # user_label â†’ user_id mapping\n",
    "    for user_idx, (user_id, gestures) in enumerate(data.items()):\n",
    "        user_label_map[user_idx] = user_id  \n",
    "        for gesture_idx, (gesture_id, files) in enumerate(gestures.items()):\n",
    "            for file_data in files:\n",
    "                labeled_data.append({\n",
    "                    \"user_label\": user_idx,\n",
    "                    \"gesture_label\": int(gesture_id)-1,\n",
    "                    \"data\": file_data\n",
    "                })\n",
    "    return labeled_data, user_label_map\n",
    "\n",
    "labeled_data, user_label_map = label_data(loaded_data)\n",
    "\n",
    "print(f\"Total labeled samples: {len(labeled_data)}\")\n",
    "num_unique_users = len(set(sample['user_label'] for sample in labeled_data))\n",
    "print(f\"num_unique_useres: {num_unique_users}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5905bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "assert \"labeled_data\" in globals(), \"labeled_dataê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € pantomime ë¡œë“œ/label_data ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\"\n",
    "assert isinstance(labeled_data, list) and len(labeled_data) > 0, \"labeled_dataê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "gesture_counts = Counter(int(s[\"gesture_label\"]) for s in labeled_data)\n",
    "\n",
    "print(\"=== Gesture (action) counts ===\")\n",
    "for g in sorted(gesture_counts):\n",
    "    print(f\"gesture {g}: {gesture_counts[g]}\")\n",
    "print(\"TOTAL:\", sum(gesture_counts.values()), \"\\n\")\n",
    "\n",
    "user_counts = Counter(int(s[\"user_label\"]) for s in labeled_data)\n",
    "\n",
    "print(\"=== User counts ===\")\n",
    "has_map = (\"user_label_map\" in globals()) and isinstance(user_label_map, dict)\n",
    "for u in sorted(user_counts):\n",
    "    if has_map and u in user_label_map:\n",
    "        print(f\"user {u} ({user_label_map[u]}): {user_counts[u]}\")\n",
    "    else:\n",
    "        print(f\"user {u}: {user_counts[u]}\")\n",
    "print(\"TOTAL:\", sum(user_counts.values()), \"\\n\")  # 41 users 210~420, 21 gestures 410~440 samples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dedf323",
   "metadata": {},
   "source": [
    "baseline preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "def ahc_upsample(data, target_size, noise_std=1e-6, seed=42):\n",
    "    \"\"\"\n",
    "    data: (N,D)\n",
    "    target_size: ëª©í‘œ ê°œìˆ˜\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    data = np.asarray(data, dtype=np.float32)\n",
    "    num_samples, dim = data.shape\n",
    "\n",
    "    if num_samples >= target_size:\n",
    "        return data\n",
    "\n",
    "    if num_samples == 0:\n",
    "        base = np.zeros((1, dim), dtype=np.float32)\n",
    "        base = np.vstack([base, base + rng.normal(scale=noise_std, size=(1, dim)).astype(np.float32)])\n",
    "        centroids = base\n",
    "    elif num_samples == 1:\n",
    "        base = data\n",
    "        base2 = base + rng.normal(scale=noise_std, size=(1, dim)).astype(np.float32)\n",
    "        centroids = np.vstack([base, base2])\n",
    "    else:\n",
    "        centroids = data.copy()\n",
    "\n",
    "    while centroids.shape[0] < target_size:\n",
    "        cluster_num = min(target_size - centroids.shape[0], max(2, centroids.shape[0] // 2))\n",
    "\n",
    "        if centroids.shape[0] < 2:\n",
    "            centroids = np.vstack([centroids, centroids + rng.normal(scale=noise_std, size=centroids.shape).astype(np.float32)])\n",
    "\n",
    "        ahc = AgglomerativeClustering(n_clusters=cluster_num)\n",
    "        labels = ahc.fit_predict(centroids)\n",
    "\n",
    "        new_centroids = []\n",
    "        for i in range(cluster_num):\n",
    "            cluster_points = centroids[labels == i]\n",
    "            if len(cluster_points) > 0:\n",
    "                new_centroid = cluster_points.mean(axis=0)\n",
    "            else:\n",
    "                new_centroid = centroids[rng.randint(len(centroids))]\n",
    "            new_centroids.append(new_centroid)\n",
    "\n",
    "        centroids = np.vstack((centroids, np.array(new_centroids, dtype=np.float32)))\n",
    "\n",
    "    return centroids[:target_size]\n",
    "\n",
    "def _as_numpy(x):\n",
    "    if torch.is_tensor(x):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return np.asarray(x)\n",
    "\n",
    "def extract_frame_list(sample_data):\n",
    "    \"\"\"\n",
    "    return: list of frames, each frame is (Pi, D) ndarray\n",
    "      - list/tuple of frames\n",
    "      - ndarray/tensor (T,P,D)\n",
    "      - ndarray/tensor (N,D) -> frame 1\n",
    "      - dict -> data/frames/points/pc \n",
    "    \"\"\"\n",
    "    if isinstance(sample_data, dict):\n",
    "        for k in [\"frames\", \"data\", \"points\", \"pc\", \"pos\", \"xyz\"]:\n",
    "            if k in sample_data:\n",
    "                sample_data = sample_data[k]\n",
    "                break\n",
    "\n",
    "    if isinstance(sample_data, (list, tuple)) and len(sample_data) > 0:\n",
    "        return [np.asarray(_as_numpy(fr), dtype=np.float32) for fr in sample_data]\n",
    "\n",
    "    arr = np.asarray(_as_numpy(sample_data), dtype=np.float32)\n",
    "    if arr.ndim == 3:  # (T,P,D)\n",
    "        return [arr[t] for t in range(arr.shape[0])]\n",
    "    if arr.ndim == 2:  # (N,D)\n",
    "        return [arr]\n",
    "    raise ValueError(f\"Unsupported sample_data: type={type(sample_data)}, shape={getattr(arr,'shape',None)}\")\n",
    "\n",
    "\n",
    "def preprocess_frames_panto(sample_data, num_frames=32, points_per_frame=32, seed=42):\n",
    "    \"\"\"\n",
    "    KMeans downsample / AHC upsample\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    frames = extract_frame_list(sample_data)\n",
    "\n",
    "    all_points = np.vstack(frames) if len(frames) > 0 else np.zeros((0, 3), dtype=np.float32)\n",
    "    if all_points.shape[1] >= 3:\n",
    "        all_points = all_points[:, :3]\n",
    "    else:\n",
    "        raise ValueError(f\"points dim < 3: {all_points.shape}\")\n",
    "\n",
    "    total_points = all_points.shape[0]\n",
    "    if total_points == 0:\n",
    "        return torch.zeros((num_frames, points_per_frame, 3), dtype=torch.float32)\n",
    "    points_per_bin = total_points // num_frames\n",
    "    remainder = total_points % num_frames\n",
    "\n",
    "    bins = []\n",
    "    start_idx = 0\n",
    "    for i in range(num_frames):\n",
    "        extra = 1 if i < remainder else 0\n",
    "        end_idx = start_idx + points_per_bin + extra\n",
    "        bins.append(all_points[start_idx:end_idx])\n",
    "        start_idx = end_idx\n",
    "    processed_frames = []\n",
    "    for fr in bins:\n",
    "        n = fr.shape[0]\n",
    "\n",
    "        if n == 0:\n",
    "            pick = all_points[rng.randint(0, total_points, size=2)]\n",
    "            fr = pick\n",
    "            n = 2\n",
    "\n",
    "        if n > points_per_frame:\n",
    "            km = KMeans(n_clusters=points_per_frame, random_state=seed, n_init=10)\n",
    "            km.fit(fr)\n",
    "            resampled = km.cluster_centers_.astype(np.float32)\n",
    "        elif n < points_per_frame:\n",
    "            resampled = ahc_upsample(fr, points_per_frame, seed=seed)\n",
    "        else:\n",
    "            resampled = fr.astype(np.float32)\n",
    "\n",
    "        processed_frames.append(resampled)\n",
    "    return torch.tensor(np.stack(processed_frames, axis=0), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def preprocess_dataset_panto(labeled_data, num_frames=32, points_per_frame=32, seed=42):\n",
    "    \"\"\"\n",
    "    labeled_data: [{\"user_label\":int, \"gesture_label\":int, \"data\":...}, ...]\n",
    "    return: list[(tensor(T,32,3), (gesture,user))]\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    total_time = 0.0\n",
    "\n",
    "    for i in range(len(labeled_data)):\n",
    "        t0 = time.time()\n",
    "        s = labeled_data[i]\n",
    "        x = s[\"data\"]\n",
    "        y = (int(s[\"gesture_label\"]), int(s[\"user_label\"]))\n",
    "\n",
    "        x_proc = preprocess_frames_panto(x, num_frames=num_frames, points_per_frame=points_per_frame, seed=seed)\n",
    "        processed.append((x_proc, y))\n",
    "\n",
    "        total_time += (time.time() - t0)\n",
    "\n",
    "    print(f\"ðŸ“Œ preprocessing time per sample: {total_time / max(len(labeled_data),1):.4f} sec\")\n",
    "    print(f\"â± total: {total_time:.2f} sec\")\n",
    "    return processed\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# save the preprocessed dataset\n",
    "# -------------------------\n",
    "import h5py\n",
    "\n",
    "def save_to_hdf5(processed, filename=\"Panto_fixed.h5\"):\n",
    "    n = len(processed)\n",
    "    sample_shape = processed[0][0].shape  # (T,32,3)\n",
    "\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        data_dset = f.create_dataset(\"data\", shape=(n, *sample_shape), dtype=\"float32\")\n",
    "        labels_dset = f.create_dataset(\"labels\", shape=(n, 2), dtype=\"int32\")\n",
    "\n",
    "        for i, (x, y) in enumerate(processed):\n",
    "            data_dset[i] = x.numpy()\n",
    "            labels_dset[i] = np.array([y[0], y[1]], dtype=np.int32)\n",
    "\n",
    "    print(f\"âœ… saved: {filename}\")\n",
    "    print(\"data:\", (n, *sample_shape), \"labels:\", (n, 2))\n",
    "\n",
    "panto_processed = preprocess_dataset_panto(labeled_data, num_frames=32, points_per_frame=32, seed=42)\n",
    "print(\"ì²« ìƒ˜í”Œ:\", panto_processed[0][0].shape, panto_processed[0][1])\n",
    "save_to_hdf5(panto_processed, filename=\"Panto_fixed.h5\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65aadd",
   "metadata": {},
   "source": [
    "PointNet style frame encoder + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaecf186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PointNetPPEncoder(nn.Module):\n",
    "    def __init__(self, out_dim=64):\n",
    "        super(PointNetPPEncoder, self).__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(3, 32, 1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(32, out_dim, 1),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_points, _ = x.shape\n",
    "        x = x.permute(0, 2, 1) \n",
    "        x = self.mlp1(x)  \n",
    "        x = self.mlp2(x)  \n",
    "        x = torch.max(x, 2, keepdim=False)[0]  \n",
    "        return x\n",
    "\n",
    "\n",
    "class PointNetPPGRU(nn.Module):\n",
    "    def __init__(self, num_classes, feature_dim=64, hidden_dim=64, num_layers=2):\n",
    "        super(PointNetPPGRU, self).__init__()\n",
    "        self.pointnet = PointNetPPEncoder(out_dim=feature_dim) \n",
    "        self.gru = nn.GRU(input_size=feature_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, num_points, _ = x.shape\n",
    "        encoded_features = []\n",
    "\n",
    "        for t in range(num_frames):\n",
    "            frame_feature = self.pointnet(x[:, t, :, :])  \n",
    "            encoded_features.append(frame_feature)\n",
    "\n",
    "        encoded_features = torch.stack(encoded_features, dim=1)  \n",
    "        # GRU \n",
    "        gru_out, _ = self.gru(encoded_features)  # (batch, 16, hidden_dim)\n",
    "        last_output = gru_out[:, -1, :] \n",
    "\n",
    "        logits = self.fc(last_output)  \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 5FOLD CV  ###########\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "FILENAME = \"Panto_fixed.h5\"   \n",
    "\n",
    "RUN_ACTION = True   \n",
    "RUN_USER   = True\n",
    "\n",
    "EXPECTED_ACTION_CLASSES = 21  #\n",
    "EXPECTED_USER_CLASSES   = 41  #\n",
    "K = 5\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "BATCH_SIZE  = 128\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY  = True\n",
    "\n",
    "EPOCHS  = 100\n",
    "LR      = 1e-3\n",
    "MAX_LR  = 3e-3\n",
    "\n",
    "PATIENCE      = 10\n",
    "MIN_DELTA     = 1e-4\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "\n",
    "MODEL_RETURNS_LOGPROB = False\n",
    "MAX_GPUS = 4\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CKPT_DIR = \"./ckpt_5fold_panto\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def load_h5_dataset(filename):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        X = np.array(f[\"data\"], dtype=np.float32)    # (N, T, P, 3)\n",
    "        Y = np.array(f[\"labels\"], dtype=np.int64)    # (N, 2) -> [gesture/action, user]\n",
    "    assert Y.ndim == 2 and Y.shape[1] >= 2, \"labels must be (N,2) with [gesture/action, user]\"\n",
    "    y_action = Y[:, 0].astype(np.int64)\n",
    "    y_user   = Y[:, 1].astype(np.int64)\n",
    "    return X, y_action, y_user\n",
    "\n",
    "\n",
    "def remap_labels(y):\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "    uniq = np.unique(y)\n",
    "    lut = {v: i for i, v in enumerate(uniq)}\n",
    "    y2 = np.array([lut[v] for v in y], dtype=np.int64)\n",
    "    return y2, int(len(uniq))\n",
    "\n",
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, data_np, labels_np):\n",
    "        self.data = np.asarray(data_np, dtype=np.float32)\n",
    "        self.labels = np.asarray(labels_np, dtype=np.int64)\n",
    "        assert len(self.data) == len(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.data[idx])          # (T,P,3)\n",
    "        y = int(self.labels[idx])\n",
    "        return x, y\n",
    "\n",
    "def stratified_kfold_indices(y, k=5, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.asarray(y)\n",
    "    folds = [[] for _ in range(k)]\n",
    "    for c in np.unique(y):\n",
    "        idx_c = np.where(y == c)[0]\n",
    "        rng.shuffle(idx_c)\n",
    "        for i, idx in enumerate(idx_c):\n",
    "            folds[i % k].append(int(idx))\n",
    "    return [np.array(f, dtype=np.int64) for f in folds]\n",
    "\n",
    "\n",
    "def stratified_train_val_split(indices, y, val_ratio=0.1, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = np.asarray(indices)\n",
    "    y_pool = y[indices]\n",
    "\n",
    "    train_idx, val_idx = [], []\n",
    "    for c in np.unique(y_pool):\n",
    "        idx_c = indices[y_pool == c].copy()\n",
    "        rng.shuffle(idx_c)\n",
    "        n_val = max(1, int(len(idx_c) * val_ratio))\n",
    "        val_idx.extend(idx_c[:n_val].tolist())\n",
    "        train_idx.extend(idx_c[n_val:].tolist())\n",
    "\n",
    "    rng.shuffle(train_idx)\n",
    "    rng.shuffle(val_idx)\n",
    "    return np.array(train_idx, dtype=np.int64), np.array(val_idx, dtype=np.int64)\n",
    "\n",
    "def wrap_model_for_multi_gpu(model: nn.Module) -> nn.Module:\n",
    "    if torch.cuda.is_available():\n",
    "        n_avail = torch.cuda.device_count()\n",
    "        if n_avail > 1:\n",
    "            n_use = min(MAX_GPUS, n_avail)\n",
    "            device_ids = list(range(n_use))\n",
    "            print(f\"Using {n_use} GPUs via DataParallel: {device_ids}\")\n",
    "            model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    return model.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for data, labels in loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        outputs = model(data)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "\n",
    "def train_one_fold(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=100,\n",
    "    lr=1e-3,\n",
    "    max_lr=3e-3,\n",
    "    patience=12,\n",
    "    min_delta=1e-4,\n",
    "    warmup_epochs=5,\n",
    "    model_returns_logprob=False\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "\n",
    "    criterion = nn.NLLLoss() if model_returns_logprob else nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for data, labels in train_loader:\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += float(loss.item())\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = correct / max(total, 1)\n",
    "        val_acc = evaluate(model, val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d}/{epochs} | Train Loss {train_loss:.4f} | Train Acc {train_acc:.4f} | Val Acc {val_acc:.4f}\")\n",
    "\n",
    "        if (val_acc - best_val_acc) > min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "            print(\"âœ… Best model updated!\")\n",
    "        else:\n",
    "            if (epoch + 1) > warmup_epochs:\n",
    "                no_improve += 1\n",
    "\n",
    "        if (epoch + 1) > warmup_epochs and no_improve >= patience:\n",
    "            print(f\"â¹ï¸ Early stopping triggered (no improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    print(f\"ðŸŽ¯ Best Val Acc: {best_val_acc:.4f}\")\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)   # âœ… bestë¡œ ë³µì›\n",
    "    return best_val_acc\n",
    "\n",
    "def run_5fold_cv(X, y_raw, task_name, expected_classes=None):\n",
    "    y, num_classes = remap_labels(y_raw)\n",
    "\n",
    "    if expected_classes is not None:\n",
    "        if num_classes != expected_classes:\n",
    "            print(f\"[WARN] {task_name}: num_classes={num_classes} (expected {expected_classes}).\")\n",
    "\n",
    "    dataset = GestureDataset(X, y)\n",
    "    folds = stratified_kfold_indices(y, k=K, seed=42)\n",
    "\n",
    "    fold_test_accs, fold_val_accs = [], []\n",
    "\n",
    "    for fold in range(K):\n",
    "        print(f\"\\n==================== {task_name.upper()} | Fold {fold+1}/{K} ====================\")\n",
    "\n",
    "        test_idx = folds[fold]\n",
    "        trainval_idx = np.concatenate([folds[i] for i in range(K) if i != fold])\n",
    "        train_idx, val_idx = stratified_train_val_split(trainval_idx, y, val_ratio=VAL_RATIO, seed=42 + fold)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            Subset(dataset, train_idx),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            persistent_workers=(NUM_WORKERS > 0)\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            Subset(dataset, val_idx),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            persistent_workers=(NUM_WORKERS > 0)\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            Subset(dataset, test_idx),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            persistent_workers=(NUM_WORKERS > 0)\n",
    "        )\n",
    "\n",
    "        model = PointNetPPGRU(num_classes)  \n",
    "        model = wrap_model_for_multi_gpu(model)\n",
    "\n",
    "        best_val_acc = train_one_fold(\n",
    "            model, train_loader, val_loader,\n",
    "            epochs=EPOCHS, lr=LR, max_lr=MAX_LR,\n",
    "            patience=PATIENCE, min_delta=MIN_DELTA, warmup_epochs=WARMUP_EPOCHS,\n",
    "            model_returns_logprob=MODEL_RETURNS_LOGPROB\n",
    "        )\n",
    "\n",
    "        ckpt_path = os.path.join(CKPT_DIR, f\"best_model_{task_name}_fold{fold+1}.pth\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"ðŸ’¾ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "        test_acc = evaluate(model, test_loader)\n",
    "        fold_val_accs.append(best_val_acc)\n",
    "        fold_test_accs.append(test_acc)\n",
    "        print(f\"ðŸ”¥ {task_name} | Fold {fold+1} Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    val_mean = float(np.mean(fold_val_accs))\n",
    "    val_std  = float(np.std(fold_val_accs, ddof=1)) if K > 1 else 0.0\n",
    "    test_mean = float(np.mean(fold_test_accs))\n",
    "    test_std  = float(np.std(fold_test_accs, ddof=1)) if K > 1 else 0.0\n",
    "\n",
    "    print(f\"\\n==================== {task_name.upper()} | 5-Fold CV Summary ====================\")\n",
    "    print(f\"#Classes: {num_classes}\")\n",
    "    print(f\"Val  Acc: {val_mean:.4f} Â± {val_std:.4f}\")\n",
    "    print(f\"Test Acc: {test_mean:.4f} Â± {test_std:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"task\": task_name,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"val_mean\": val_mean, \"val_std\": val_std,\n",
    "        \"test_mean\": test_mean, \"test_std\": test_std\n",
    "    }\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "X, y_action, y_user = load_h5_dataset(FILENAME)\n",
    "\n",
    "print(\"[INFO] Loaded:\", FILENAME)\n",
    "print(\"[INFO] X:\", X.shape, \"action unique:\", len(np.unique(y_action)), \"user unique:\", len(np.unique(y_user)))\n",
    "\n",
    "results = []\n",
    "if RUN_ACTION:\n",
    "    results.append(run_5fold_cv(X, y_action, \"action\", expected_classes=EXPECTED_ACTION_CLASSES))\n",
    "if RUN_USER:\n",
    "    results.append(run_5fold_cv(X, y_user, \"user\", expected_classes=EXPECTED_USER_CLASSES))\n",
    "\n",
    "print(\"\\n==================== Overall Summary ====================\")\n",
    "for r in results:\n",
    "    print(f\"{r['task']:>6s} | C={r['num_classes']:>3d} | Val {r['val_mean']:.4f}Â±{r['val_std']:.4f} | Test {r['test_mean']:.4f}Â±{r['test_std']:.4f}\") #9552/9144\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa10f9af",
   "metadata": {},
   "source": [
    "DGCNN + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5820f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import DynamicEdgeConv, global_max_pool\n",
    "\n",
    "\n",
    "def MLP(channels):\n",
    "    layers = []\n",
    "    for i in range(1, len(channels)):\n",
    "        layers.append(nn.Linear(channels[i - 1], channels[i]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(channels[i]))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class DGCNNEncoder(nn.Module):\n",
    "    def __init__(self, out_dim=64, k=16):\n",
    "        super(DGCNNEncoder, self).__init__()\n",
    "        self.conv1 = DynamicEdgeConv(MLP([2 * 3, 64, 64, 64]), k, aggr='max')\n",
    "        self.conv2 = DynamicEdgeConv(MLP([2 * 64, 128]), k, aggr='max')\n",
    "        self.lin1 = MLP([128 + 64, out_dim])  \n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        x1 = self.conv1(x, batch)\n",
    "        x2 = self.conv2(x1, batch)\n",
    "        out = self.lin1(torch.cat([x1, x2], dim=1))  \n",
    "        out = global_max_pool(out, batch) \n",
    "        return out  \n",
    "\n",
    "class DGCNNGRU(nn.Module):\n",
    "    def __init__(self, num_classes, feature_dim=64, hidden_dim=64, num_layers=2, k=16):\n",
    "        super(DGCNNGRU, self).__init__()\n",
    "        self.dgcnn = DGCNNEncoder(out_dim=feature_dim, k=k)\n",
    "        self.gru = nn.GRU(input_size=feature_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        batch_size, num_frames, num_points, _ = x.shape\n",
    "        encoded_features = []\n",
    "\n",
    "        for t in range(num_frames):\n",
    "            frame_x = x[:, t, :, :].reshape(-1, 3) \n",
    "            frame_batch = torch.arange(batch_size, device=x.device).repeat_interleave(num_points)  \n",
    "\n",
    "            frame_feature = self.dgcnn(frame_x, frame_batch) \n",
    "            encoded_features.append(frame_feature)\n",
    "        encoded_features = torch.stack(encoded_features, dim=1)  \n",
    "\n",
    "        gru_out, _ = self.gru(encoded_features)  # (batch, 16, hidden_dim)\n",
    "        last_output = gru_out[:, -1, :]  \n",
    "\n",
    "        logits = self.fc(last_output)  # (batch, num_classes)\n",
    "        return F.log_softmax(logits, dim=-1) \n",
    "    \n",
    "class DGCNNWithAutoBatchIdx(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "    def forward(self, x):\n",
    "        B, T, N, _ = x.shape\n",
    "        batch_idx = torch.arange(B, device=x.device).repeat_interleave(N)\n",
    "        return self.base(x, batch_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "H5_PATH   = \"Panto_fixed.h5\"\n",
    "CKPT_DIR  = \"./ckpt_5fold_dgcnn_panto\"\n",
    "MODEL_TAG = \"dgcnn\"                \n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "RUN_ACTION = True\n",
    "RUN_USER   = True\n",
    "\n",
    "K = 5\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "BATCH_SIZE  = 16\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY  = True\n",
    "\n",
    "EPOCHS  = 100\n",
    "LR      = 1e-3\n",
    "MAX_LR  = 3e-3\n",
    "PATIENCE      = 12\n",
    "MIN_DELTA     = 1e-4\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "MODEL_RETURNS_LOGPROB = True\n",
    "\n",
    "MAX_GPUS = 4\n",
    "\n",
    "EXPECTED_GESTURE_CLASSES = 21\n",
    "EXPECTED_USER_CLASSES    = 41\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def make_model(num_classes: int) -> nn.Module:\n",
    "    return DGCNNWithAutoBatchIdx(DGCNNGRU(num_classes)) \n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def load_h5_panto(path):\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        X = np.array(f[\"data\"])       # (N,T,P,3)\n",
    "        Y = np.array(f[\"labels\"])     # (N,2)\n",
    "    y_gesture = Y[:, 0].astype(np.int64)\n",
    "    y_user    = Y[:, 1].astype(np.int64)\n",
    "    return X, y_gesture, y_user\n",
    "\n",
    "def remap_labels(y_raw):\n",
    "    y_raw = np.asarray(y_raw, dtype=np.int64)\n",
    "    uniq = np.unique(y_raw)\n",
    "    lut = {v: i for i, v in enumerate(uniq)}\n",
    "    y = np.array([lut[v] for v in y_raw], dtype=np.int64)\n",
    "    return y, int(len(uniq))\n",
    "\n",
    "def stratified_kfold_indices(y, k=5, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.asarray(y)\n",
    "    folds = [[] for _ in range(k)]\n",
    "    for c in np.unique(y):\n",
    "        idx_c = np.where(y == c)[0]\n",
    "        rng.shuffle(idx_c)\n",
    "        for i, idx in enumerate(idx_c):\n",
    "            folds[i % k].append(int(idx))\n",
    "    return [np.array(f, dtype=np.int64) for f in folds]\n",
    "\n",
    "def stratified_train_val_split(indices, y, val_ratio=0.1, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = np.asarray(indices)\n",
    "    y_pool = y[indices]\n",
    "    train_idx, val_idx = [], []\n",
    "    for c in np.unique(y_pool):\n",
    "        idx_c = indices[y_pool == c].copy()\n",
    "        rng.shuffle(idx_c)\n",
    "        n_val = max(1, int(len(idx_c) * val_ratio))\n",
    "        val_idx.extend(idx_c[:n_val].tolist())\n",
    "        train_idx.extend(idx_c[n_val:].tolist())\n",
    "    rng.shuffle(train_idx); rng.shuffle(val_idx)\n",
    "    return np.array(train_idx, dtype=np.int64), np.array(val_idx, dtype=np.int64)\n",
    "\n",
    "class H5TensorDataset(Dataset):\n",
    "    def __init__(self, X_np, y_np):\n",
    "        self.X = X_np\n",
    "        self.y = y_np\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return torch.from_numpy(self.X[i]).float(), int(self.y[i])\n",
    "\n",
    "def wrap_dp(model: nn.Module) -> nn.Module:\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1 and MAX_GPUS > 1:\n",
    "        n_use = min(MAX_GPUS, torch.cuda.device_count())\n",
    "        model = nn.DataParallel(model, device_ids=list(range(n_use)))\n",
    "        print(f\"[DP] Using {n_use} GPUs\")\n",
    "    return model.to(device)\n",
    "\n",
    "def get_base_state_dict(model: nn.Module) -> dict:\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model = model.module\n",
    "    return model.state_dict()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_acc(model, loader):\n",
    "    model.eval()\n",
    "    ok, tot = 0, 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = torch.as_tensor(y, dtype=torch.long, device=device)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1)\n",
    "        ok += (pred == y).sum().item()\n",
    "        tot += y.numel()\n",
    "    return ok / max(tot, 1)\n",
    "\n",
    "def train_one_fold(model, train_loader, val_loader):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=MAX_LR, steps_per_epoch=len(train_loader), epochs=EPOCHS)\n",
    "    criterion = nn.NLLLoss() if MODEL_RETURNS_LOGPROB else nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = torch.as_tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            tr_loss += float(loss.item())\n",
    "\n",
    "        va = evaluate_acc(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1:03d}/{EPOCHS} | TrainLoss {tr_loss:.4f} | ValAcc {va:.4f}\")\n",
    "\n",
    "        if (va - best_val) > MIN_DELTA:\n",
    "            best_val = va\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "            print(\"âœ… Best updated\")\n",
    "        else:\n",
    "            if (epoch + 1) > WARMUP_EPOCHS:\n",
    "                no_improve += 1\n",
    "\n",
    "        if (epoch + 1) > WARMUP_EPOCHS and no_improve >= PATIENCE:\n",
    "            print(f\"â¹ï¸ Early stop ({PATIENCE} epochs no improve)\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return best_val\n",
    "\n",
    "def run_5fold_cv(X, y_raw, task_name: str):\n",
    "    y, num_classes = remap_labels(y_raw)\n",
    "    ds = H5TensorDataset(X, y)\n",
    "    folds = stratified_kfold_indices(y, k=K, seed=42)\n",
    "\n",
    "    fold_val, fold_test = [], []\n",
    "\n",
    "    for fold in range(K):\n",
    "        print(f\"\\n==================== {MODEL_TAG.upper()} | {task_name.upper()} | Fold {fold+1}/{K} ====================\")\n",
    "        test_idx = folds[fold]\n",
    "        trainval_idx = np.concatenate([folds[i] for i in range(K) if i != fold])\n",
    "        train_idx, val_idx = stratified_train_val_split(trainval_idx, y, val_ratio=VAL_RATIO, seed=42 + fold)\n",
    "\n",
    "        train_loader = DataLoader(Subset(ds, train_idx), batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                                  persistent_workers=(NUM_WORKERS > 0))\n",
    "        val_loader   = DataLoader(Subset(ds, val_idx), batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                                  persistent_workers=(NUM_WORKERS > 0))\n",
    "        test_loader  = DataLoader(Subset(ds, test_idx), batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                                  persistent_workers=(NUM_WORKERS > 0))\n",
    "\n",
    "        model = wrap_dp(make_model(num_classes))\n",
    "\n",
    "        best_val = train_one_fold(model, train_loader, val_loader)\n",
    "\n",
    "        ckpt_path = os.path.join(CKPT_DIR, f\"best_model_{MODEL_TAG}_{task_name}_fold{fold+1}.pth\")\n",
    "        torch.save(get_base_state_dict(model), ckpt_path)\n",
    "        print(f\"ðŸ’¾ Saved: {ckpt_path}\")\n",
    "\n",
    "        te = evaluate_acc(model, test_loader)\n",
    "        fold_val.append(best_val)\n",
    "        fold_test.append(te)\n",
    "        print(f\"ðŸ”¥ Fold{fold+1} TestAcc: {te:.4f}\")\n",
    "\n",
    "    val_mean = float(np.mean(fold_val))\n",
    "    val_std  = float(np.std(fold_val, ddof=1)) if K > 1 else 0.0\n",
    "    test_mean = float(np.mean(fold_test))\n",
    "    test_std  = float(np.std(fold_test, ddof=1)) if K > 1 else 0.0\n",
    "\n",
    "    print(f\"\\n==================== {MODEL_TAG.upper()} | {task_name.upper()} | Summary ====================\")\n",
    "    print(f\"#Classes: {num_classes}\")\n",
    "    print(f\"Val  Acc: {val_mean:.4f} Â± {val_std:.4f}\")\n",
    "    print(f\"Test Acc: {test_mean:.4f} Â± {test_std:.4f}\")\n",
    "\n",
    "    return {\"task\": task_name, \"num_classes\": num_classes,\n",
    "            \"val_mean\": val_mean, \"val_std\": val_std, \"test_mean\": test_mean, \"test_std\": test_std}\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "X, y_gesture_raw, y_user_raw = load_h5_panto(H5_PATH)\n",
    "print(f\"[INFO] X={X.shape}, gesture uniq={len(np.unique(y_gesture_raw))}, user uniq={len(np.unique(y_user_raw))}\")\n",
    "\n",
    "if EXPECTED_GESTURE_CLASSES is not None:\n",
    "    assert len(np.unique(y_gesture_raw)) == EXPECTED_GESTURE_CLASSES\n",
    "if EXPECTED_USER_CLASSES is not None:\n",
    "    assert len(np.unique(y_user_raw)) == EXPECTED_USER_CLASSES\n",
    "\n",
    "results = []\n",
    "if RUN_ACTION:\n",
    "    results.append(run_5fold_cv(X, y_gesture_raw, \"action\"))\n",
    "if RUN_USER:\n",
    "    results.append(run_5fold_cv(X, y_user_raw, \"user\"))\n",
    "\n",
    "print(\"\\n==================== Overall Summary ====================\")\n",
    "for r in results:\n",
    "    print(f\"{r['task']:>6s} | C={r['num_classes']:>3d} | \"\n",
    "          f\"Val {r['val_mean']:.4f}Â±{r['val_std']:.4f} | \"\n",
    "          f\"Test {r['test_mean']:.4f}Â±{r['test_std']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8fbb8",
   "metadata": {},
   "source": [
    "PointTransformerV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "os.environ.setdefault(\"SPCONV_ALLOW_TF32\", \"1\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "from model import PointTransformerV3\n",
    "\n",
    "assert \"labeled_data\" in globals() and isinstance(labeled_data, list) and len(labeled_data) > 0, \\\n",
    "    \"load pantomime\"\n",
    "\n",
    "RUN_ACTION = True\n",
    "RUN_USER   = True\n",
    "\n",
    "K_FOLDS   = 5\n",
    "VAL_RATIO = 0.1  \n",
    "FOLD_SEED = 42\n",
    "VAL_SEED  = 123\n",
    "\n",
    "BATCH_SIZE  = 4\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY  = True\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "BASE_LR = 3e-4\n",
    "MAX_LR  = 1e-3\n",
    "MIN_LR  = 1e-5\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "PATIENCE  = 12\n",
    "MIN_DELTA = 1e-4\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "WEIGHT_DECAY   = 1e-4\n",
    "\n",
    "GRID_SIZE = 0.08\n",
    "USE_T_SINCOS = True\n",
    "\n",
    "USE_AMP = True\n",
    "USE_DATAPARALLEL = False\n",
    "MAX_GPUS = 4\n",
    "MAX_POINTS = 8192\n",
    "PREFETCH = 4\n",
    "\n",
    "DETERMINISTIC_SUBSAMPLE = True\n",
    "SUBSAMPLE_SEED = 2025\n",
    "\n",
    "USE_BALANCED_SUBSET = False\n",
    "PER_GESTURE = 450\n",
    "BALANCE_SEED = 12\n",
    "\n",
    "CKPT_DIR  = \"./ckpt_ptv3_panto_5fold_stable\"\n",
    "MODEL_TAG = \"ptv3\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def remap_labels(y_raw):\n",
    "    y_raw = np.asarray(y_raw, dtype=np.int64)\n",
    "    uniq = np.unique(y_raw)\n",
    "    lut = {v: i for i, v in enumerate(uniq)}\n",
    "    y = np.array([lut[v] for v in y_raw], dtype=np.int64)\n",
    "    return y, int(len(uniq))\n",
    "\n",
    "def stratified_train_val_split(pos_indices, y, val_ratio=0.1, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_indices = np.asarray(pos_indices, dtype=np.int64)\n",
    "    y_pool = y[pos_indices]\n",
    "    train, val = [], []\n",
    "    for c in np.unique(y_pool):\n",
    "        idx_c = pos_indices[y_pool == c].copy()\n",
    "        rng.shuffle(idx_c)\n",
    "        n_val = max(1, int(len(idx_c) * val_ratio))\n",
    "        val.extend(idx_c[:n_val].tolist())\n",
    "        train.extend(idx_c[n_val:].tolist())\n",
    "    rng.shuffle(train); rng.shuffle(val)\n",
    "    return np.array(train, dtype=np.int64), np.array(val, dtype=np.int64)\n",
    "\n",
    "def make_stratified_folds(pos_indices, y, k=5, seed=42):\n",
    "    \"\"\"\n",
    "    pos_indices: 0..(pool-1) positions\n",
    "    y: labels on pool positions (already remapped)\n",
    "    return: folds = [np.array(test_pos_for_fold0), ... fold(k-1)]\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_indices = np.asarray(pos_indices, dtype=np.int64)\n",
    "    y_pool = y[pos_indices]\n",
    "\n",
    "    by_class = {}\n",
    "    for c in np.unique(y_pool):\n",
    "        idx_c = pos_indices[y_pool == c].copy()\n",
    "        rng.shuffle(idx_c)\n",
    "        by_class[int(c)] = idx_c\n",
    "\n",
    "    folds = [[] for _ in range(k)]\n",
    "    for c, idx_c in by_class.items():\n",
    "        parts = np.array_split(idx_c, k)\n",
    "        for f in range(k):\n",
    "            folds[f].extend(parts[f].tolist())\n",
    "\n",
    "    folds = [np.array(folds[f], dtype=np.int64) for f in range(k)]\n",
    "    for f in range(k):\n",
    "        rng.shuffle(folds[f])\n",
    "    return folds\n",
    "\n",
    "\n",
    "def _get_x_from_item(item):\n",
    "    x = item.get(\"data\", item)\n",
    "    if isinstance(x, dict):\n",
    "        for k in [\"pos\", \"points\", \"pc\", \"xyz\", \"coords\", \"data\", \"frames\"]:\n",
    "            if k in x:\n",
    "                x = x[k]\n",
    "                break\n",
    "    return x\n",
    "\n",
    "class PantoIndexLabelDataset(Dataset):\n",
    "    def __init__(self, base_list, idxs, labels_mapped):\n",
    "        self.base = base_list\n",
    "        self.idxs = np.asarray(idxs, dtype=np.int64)\n",
    "        self.labels = np.asarray(labels_mapped, dtype=np.int64)\n",
    "        assert len(self.idxs) == len(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        base_idx = int(self.idxs[i])\n",
    "        item = self.base[base_idx]\n",
    "        x = _get_x_from_item(item)\n",
    "        y = int(self.labels[i])\n",
    "        return x, y, base_idx\n",
    "\n",
    "def _cap_points(coord, feat, max_points, seed_int=None):\n",
    "    if (max_points is None) or (coord.size(0) <= max_points):\n",
    "        return coord, feat\n",
    "    n = coord.size(0)\n",
    "\n",
    "    if DETERMINISTIC_SUBSAMPLE and (seed_int is not None):\n",
    "        g = torch.Generator(device=\"cpu\")\n",
    "        g.manual_seed(int(SUBSAMPLE_SEED + int(seed_int)) % (2**31 - 1))\n",
    "        idx = torch.randperm(n, generator=g)[:max_points]\n",
    "    else:\n",
    "        idx = torch.randperm(n)[:max_points]\n",
    "\n",
    "    return coord[idx], feat[idx]\n",
    "\n",
    "def _to_points_flat(x, use_t_sincos=True, max_points=MAX_POINTS, sample_id=None):\n",
    "    if isinstance(x, dict):\n",
    "        for k in [\"pos\",\"points\",\"pc\",\"xyz\",\"coords\",\"data\",\"frames\"]:\n",
    "            if k in x:\n",
    "                x = x[k]\n",
    "                break\n",
    "\n",
    "    # list of frames\n",
    "    if isinstance(x, (list, tuple)) and len(x) > 0 and (not torch.is_tensor(x)) and (not isinstance(x, np.ndarray)):\n",
    "        coords, feats = [], []\n",
    "        for t, fr in enumerate(x):\n",
    "            if fr is None:\n",
    "                continue\n",
    "            if isinstance(fr, dict):\n",
    "                for k in [\"pos\",\"points\",\"pc\",\"xyz\",\"coords\",\"data\"]:\n",
    "                    if k in fr:\n",
    "                        fr = fr[k]\n",
    "                        break\n",
    "            fr = torch.as_tensor(fr, dtype=torch.float32)\n",
    "            if fr.ndim != 2 or fr.size(-1) < 3:\n",
    "                continue\n",
    "            coord = fr[:, :3].contiguous()\n",
    "            if coord.size(0) == 0:\n",
    "                continue\n",
    "            if use_t_sincos:\n",
    "                s = math.sin(float(t))\n",
    "                c = math.cos(float(t))\n",
    "                pe = coord.new_empty((coord.size(0), 2))\n",
    "                pe[:, 0] = s\n",
    "                pe[:, 1] = c\n",
    "                feat = torch.cat([coord, pe], dim=1)\n",
    "            else:\n",
    "                feat = coord\n",
    "            coords.append(coord); feats.append(feat)\n",
    "\n",
    "        if len(coords) == 0:\n",
    "            return torch.zeros((0,3), dtype=torch.float32), torch.zeros((0,5 if use_t_sincos else 3), dtype=torch.float32)\n",
    "\n",
    "        coord_i = torch.cat(coords, dim=0)\n",
    "        feat_i  = torch.cat(feats,  dim=0)\n",
    "        coord_i, feat_i = _cap_points(coord_i, feat_i, max_points, seed_int=sample_id)\n",
    "        return coord_i, feat_i\n",
    "\n",
    "    x = torch.as_tensor(x, dtype=torch.float32)\n",
    "\n",
    "    # (T,P,C)\n",
    "    if x.ndim == 3 and x.size(-1) >= 3:\n",
    "        T, P, _ = x.shape\n",
    "        coord = x[..., :3].reshape(-1, 3).contiguous()\n",
    "        if use_t_sincos:\n",
    "            t = torch.arange(T, dtype=torch.float32)\n",
    "            pe_t = torch.stack([torch.sin(t), torch.cos(t)], dim=1)\n",
    "            pe = pe_t.repeat_interleave(P, dim=0)\n",
    "            feat = torch.cat([coord, pe.to(coord.device)], dim=1)\n",
    "        else:\n",
    "            feat = coord\n",
    "        coord, feat = _cap_points(coord, feat, max_points, seed_int=sample_id)\n",
    "        return coord, feat\n",
    "\n",
    "    # (N,C)\n",
    "    if x.ndim == 2 and x.size(-1) >= 3:\n",
    "        coord = x[:, :3].contiguous()\n",
    "        if use_t_sincos:\n",
    "            pe = coord.new_empty((coord.size(0), 2))\n",
    "            pe[:, 0] = 0.0\n",
    "            pe[:, 1] = 1.0\n",
    "            feat = torch.cat([coord, pe], dim=1)\n",
    "        else:\n",
    "            feat = coord\n",
    "        coord, feat = _cap_points(coord, feat, max_points, seed_int=sample_id)\n",
    "        return coord, feat\n",
    "\n",
    "    raise ValueError(f\"Unsupported sample shape/type: type={type(x)} shape={getattr(x, 'shape', None)}\")\n",
    "\n",
    "def ptv3_collate_cls(batch):\n",
    "    # batch: (x, y, sample_id)\n",
    "    B = len(batch)\n",
    "    coords_i, feats_i, lens, labels = [], [], [], []\n",
    "\n",
    "    for (x, y, sid) in batch:\n",
    "        coord, feat = _to_points_flat(x, use_t_sincos=USE_T_SINCOS, max_points=MAX_POINTS, sample_id=int(sid))\n",
    "        if coord.numel() == 0:\n",
    "            raise ValueError(\"Empty point cloud (Ni=0). í•´ë‹¹ ìƒ˜í”Œ ì œê±°/ì²˜ë¦¬ í•„ìš”.\")\n",
    "        coords_i.append(coord)\n",
    "        feats_i.append(feat)\n",
    "        lens.append(coord.size(0))\n",
    "        labels.append(int(y))\n",
    "\n",
    "    Nmax = int(max(lens))\n",
    "    Cin  = int(feats_i[0].size(1))\n",
    "\n",
    "    coord_pad = torch.zeros((B, Nmax, 3), dtype=torch.float32)\n",
    "    feat_pad  = torch.zeros((B, Nmax, Cin), dtype=torch.float32)\n",
    "    lengths   = torch.tensor(lens, dtype=torch.long)\n",
    "    labels    = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    for b in range(B):\n",
    "        n = coords_i[b].size(0)\n",
    "        coord_pad[b, :n] = coords_i[b]\n",
    "        feat_pad[b, :n]  = feats_i[b]\n",
    "\n",
    "    batch_dict = {\"coord\": coord_pad, \"feat\": feat_pad, \"lengths\": lengths, \"grid_size\": float(GRID_SIZE)}\n",
    "    return batch_dict, labels\n",
    "\n",
    "class PTv3Classifier(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels):\n",
    "        super().__init__()\n",
    "        self.backbone = PointTransformerV3(\n",
    "            in_channels=in_channels,\n",
    "            cls_mode=True,\n",
    "            enable_flash=False,\n",
    "        )\n",
    "        self.head = nn.LazyLinear(num_classes)\n",
    "\n",
    "    def forward(self, batch_dict):\n",
    "        coord = batch_dict[\"coord\"]\n",
    "        feat  = batch_dict[\"feat\"]\n",
    "        lengths = batch_dict[\"lengths\"]\n",
    "        grid_size = batch_dict[\"grid_size\"]\n",
    "\n",
    "        B, Nmax, _ = coord.shape\n",
    "        ar = torch.arange(Nmax, device=coord.device).unsqueeze(0)\n",
    "        mask = ar < lengths.unsqueeze(1)\n",
    "\n",
    "        coord_flat = coord[mask]\n",
    "        feat_flat  = feat[mask]\n",
    "        batch = torch.repeat_interleave(torch.arange(B, device=coord.device), lengths)\n",
    "\n",
    "        data_dict = {\"coord\": coord_flat, \"feat\": feat_flat, \"batch\": batch, \"grid_size\": float(grid_size)}\n",
    "\n",
    "        point = self.backbone(data_dict)\n",
    "        featN = point.feat\n",
    "        batchN = point.batch\n",
    "\n",
    "        C = featN.size(1)\n",
    "        pooled = featN.new_zeros((B, C))\n",
    "        cnt = featN.new_zeros((B, 1))\n",
    "        pooled.index_add_(0, batchN, featN)\n",
    "        cnt.index_add_(0, batchN, torch.ones((featN.size(0), 1), device=featN.device, dtype=featN.dtype))\n",
    "        pooled = pooled / cnt.clamp_min(1.0)\n",
    "\n",
    "        return self.head(pooled)\n",
    "\n",
    "def wrap_model(model: nn.Module) -> nn.Module:\n",
    "    if USE_DATAPARALLEL and torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        n_use = min(MAX_GPUS, torch.cuda.device_count())\n",
    "        model = nn.DataParallel(model, device_ids=list(range(n_use)))\n",
    "        print(f\"[DP] Using {n_use} GPUs\")\n",
    "    return model.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok, tot = 0, 0\n",
    "    for bd, y in loader:\n",
    "        for k, v in bd.items():\n",
    "            if torch.is_tensor(v):\n",
    "                bd[k] = v.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits = model(bd)\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.numel()\n",
    "    return ok / max(tot, 1)\n",
    "\n",
    "def set_lr(optimizer, lr: float):\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg[\"lr\"] = lr\n",
    "\n",
    "def warmup_cosine_lr(step, total_steps, warmup_steps, base_lr, max_lr, min_lr):\n",
    "    if step < warmup_steps:\n",
    "        return base_lr + (max_lr - base_lr) * (step / max(1, warmup_steps))\n",
    "    t = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "    t = min(max(t, 0.0), 1.0)\n",
    "    return min_lr + 0.5 * (max_lr - min_lr) * (1.0 + math.cos(math.pi * t))\n",
    "\n",
    "def train_one_run(model, train_loader, val_loader):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    use_amp = bool(USE_AMP and device.type == \"cuda\")\n",
    "    use_bf16 = bool(use_amp and torch.cuda.is_bf16_supported())\n",
    "    amp_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(use_amp and (not use_bf16)))\n",
    "\n",
    "    total_steps  = EPOCHS * len(train_loader)\n",
    "    warmup_steps = WARMUP_EPOCHS * len(train_loader)\n",
    "    global_step = 0\n",
    "\n",
    "    best_val = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "    first_step_printed = False\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        tr_loss, tr_ok, tr_tot = 0.0, 0, 0\n",
    "\n",
    "        for bd, y in train_loader:\n",
    "            lr = warmup_cosine_lr(global_step, total_steps, warmup_steps, BASE_LR, MAX_LR, MIN_LR)\n",
    "            set_lr(optimizer, lr)\n",
    "            global_step += 1\n",
    "\n",
    "            for k, v in bd.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    bd[k] = v.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            if (not first_step_printed) and device.type == \"cuda\":\n",
    "                first_step_printed = True\n",
    "                print(f\"[GPU CHECK] coord={bd['coord'].device}, feat={bd['feat'].device}, y={y.device} | AMP={use_amp} dtype={amp_dtype}\")\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=amp_dtype, enabled=use_amp):\n",
    "                logits = model(bd)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "            if not torch.isfinite(loss):\n",
    "                print(f\"[WARN] non-finite loss detected. skip step. loss={float(loss)}\")\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                continue\n",
    "\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                if GRAD_CLIP_NORM and GRAD_CLIP_NORM > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if GRAD_CLIP_NORM and GRAD_CLIP_NORM > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "                optimizer.step()\n",
    "\n",
    "            tr_loss += float(loss.item())\n",
    "            tr_ok += (logits.argmax(1) == y).sum().item()\n",
    "            tr_tot += y.numel()\n",
    "\n",
    "        tr_acc = tr_ok / max(tr_tot, 1)\n",
    "        va_acc = evaluate(model, val_loader)\n",
    "        cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d}/{EPOCHS} | LR {cur_lr:.2e} | TrainLoss {tr_loss:.4f} | TrainAcc {tr_acc:.4f} | ValAcc {va_acc:.4f}\")\n",
    "\n",
    "        if (va_acc - best_val) > MIN_DELTA:\n",
    "            best_val = va_acc\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "            print(\"âœ… Best updated\")\n",
    "        else:\n",
    "            if (epoch + 1) > WARMUP_EPOCHS:\n",
    "                no_improve += 1\n",
    "\n",
    "        if (epoch + 1) > WARMUP_EPOCHS and no_improve >= PATIENCE:\n",
    "            print(f\"â¹ï¸ Early stop ({PATIENCE} epochs no improve)\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return best_val\n",
    "\n",
    "def speed_check(train_loader, model, steps=6):\n",
    "    it = iter(train_loader)\n",
    "    t0 = time.time()\n",
    "    batches = []\n",
    "    for _ in range(steps):\n",
    "        batches.append(next(it))\n",
    "    t1 = time.time()\n",
    "\n",
    "    bd, y = batches[-1]\n",
    "    for k, v in bd.items():\n",
    "        if torch.is_tensor(v):\n",
    "            bd[k] = v.to(device, non_blocking=True)\n",
    "    y = y.to(device, non_blocking=True)\n",
    "\n",
    "    model.train()\n",
    "    opt = optim.SGD(model.parameters(), lr=1e-6)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t2 = time.time()\n",
    "    for _ in range(steps):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        out = model(bd)\n",
    "        loss = out.mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t3 = time.time()\n",
    "\n",
    "    print(f\"[SPEED] dataload+collate: {(t1-t0)/steps*1000:.2f} ms/step | fwd+bwd: {(t3-t2)/steps*1000:.2f} ms/step\")\n",
    "\n",
    "all_indices = np.arange(len(labeled_data), dtype=np.int64)\n",
    "\n",
    "if USE_BALANCED_SUBSET:\n",
    "    rng = np.random.RandomState(BALANCE_SEED)\n",
    "    by_g = defaultdict(list)\n",
    "    for i, s in enumerate(labeled_data):\n",
    "        by_g[int(s[\"gesture_label\"])].append(i)\n",
    "    pool = []\n",
    "    for g in sorted(by_g.keys()):\n",
    "        idxs = by_g[g]\n",
    "        rng.shuffle(idxs)\n",
    "        pool.extend(idxs[:min(PER_GESTURE, len(idxs))])\n",
    "    pool_indices = np.array(sorted(pool), dtype=np.int64)\n",
    "    print(f\"[BALANCED] pool={len(pool_indices)} (PER_GESTURE={PER_GESTURE})\")\n",
    "else:\n",
    "    pool_indices = all_indices\n",
    "    print(f\"[POOL] pool={len(pool_indices)} (all)\")\n",
    "\n",
    "def run_kfold(task_name: str):\n",
    "    if task_name not in [\"action\", \"user\"]:\n",
    "        raise ValueError(task_name)\n",
    "\n",
    "    y_raw = np.array(\n",
    "        [int(labeled_data[i][\"gesture_label\"] if task_name == \"action\" else labeled_data[i][\"user_label\"])\n",
    "         for i in pool_indices],\n",
    "        dtype=np.int64\n",
    "    )\n",
    "    y, num_classes = remap_labels(y_raw)\n",
    "\n",
    "    print(f\"\\n[{task_name.upper()}] #classes={num_classes} | pool={len(pool_indices)} | K={K_FOLDS}\")\n",
    "\n",
    "    all_pos = np.arange(len(pool_indices), dtype=np.int64)\n",
    "    folds = make_stratified_folds(all_pos, y, k=K_FOLDS, seed=FOLD_SEED)\n",
    "\n",
    "    fold_results = []\n",
    "\n",
    "    for fold in range(K_FOLDS):\n",
    "        test_pos = folds[fold]\n",
    "        trainval_pos = np.setdiff1d(all_pos, test_pos, assume_unique=False)\n",
    "        train_pos, val_pos = stratified_train_val_split(trainval_pos, y, val_ratio=VAL_RATIO, seed=VAL_SEED + fold)\n",
    "\n",
    "        train_idx = pool_indices[train_pos]\n",
    "        val_idx   = pool_indices[val_pos]\n",
    "        test_idx  = pool_indices[test_pos]\n",
    "\n",
    "        ds_train = PantoIndexLabelDataset(labeled_data, train_idx, y[train_pos])\n",
    "        ds_val   = PantoIndexLabelDataset(labeled_data, val_idx,   y[val_pos])\n",
    "        ds_test  = PantoIndexLabelDataset(labeled_data, test_idx,  y[test_pos])\n",
    "\n",
    "        loader_kw = dict(\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            persistent_workers=(NUM_WORKERS > 0),\n",
    "            prefetch_factor=PREFETCH if NUM_WORKERS > 0 else None,\n",
    "            collate_fn=ptv3_collate_cls,\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,  **loader_kw)\n",
    "        val_loader   = DataLoader(ds_val,   batch_size=BATCH_SIZE, shuffle=False, **loader_kw)\n",
    "        test_loader  = DataLoader(ds_test,  batch_size=BATCH_SIZE, shuffle=False, **loader_kw)\n",
    "\n",
    "        in_ch = 5 if USE_T_SINCOS else 3\n",
    "        model = wrap_model(PTv3Classifier(num_classes=num_classes, in_channels=in_ch))\n",
    "\n",
    "        print(f\"\\n----- {task_name.upper()} | FOLD {fold}/{K_FOLDS-1} -----\")\n",
    "        print(f\"[SPLIT] train/val/test = {len(ds_train)}/{len(ds_val)}/{len(ds_test)}\")\n",
    "        print(f\"[STABLE CFG] AMP={USE_AMP} | bf16={torch.cuda.is_available() and torch.cuda.is_bf16_supported()} | \"\n",
    "              f\"MAX_POINTS={MAX_POINTS} (deterministic={DETERMINISTIC_SUBSAMPLE}) | GRID_SIZE={GRID_SIZE} | \"\n",
    "              f\"LR warmup+cosine (base={BASE_LR}, max={MAX_LR}, min={MIN_LR}) | CLIP={GRAD_CLIP_NORM} | wd={WEIGHT_DECAY}\")\n",
    "\n",
    "        if device.type == \"cuda\" and fold == 0:\n",
    "            speed_check(train_loader, model, steps=6)\n",
    "\n",
    "        best_val = train_one_run(model, train_loader, val_loader)\n",
    "\n",
    "        ckpt_path = os.path.join(CKPT_DIR, f\"best_model_{MODEL_TAG}_{task_name}_fold{fold}.pth\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"ðŸ’¾ Saved: {ckpt_path}\")\n",
    "\n",
    "        te = evaluate(model, test_loader)\n",
    "        print(f\"ðŸ”¥ {task_name} | Fold {fold} | BestVal {best_val:.4f} | TestAcc {te:.4f}\")\n",
    "\n",
    "        fold_results.append({\n",
    "            \"task\": task_name,\n",
    "            \"fold\": int(fold),\n",
    "            \"num_classes\": int(num_classes),\n",
    "            \"best_val\": float(best_val),\n",
    "            \"test_acc\": float(te),\n",
    "            \"sizes\": {\"train\": len(ds_train), \"val\": len(ds_val), \"test\": len(ds_test)}\n",
    "        })\n",
    "\n",
    "        del model, train_loader, val_loader, test_loader, ds_train, ds_val, ds_test\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    test_accs = [r[\"test_acc\"] for r in fold_results]\n",
    "    mean_acc = float(np.mean(test_accs)) if len(test_accs) else 0.0\n",
    "    std_acc  = float(np.std(test_accs))  if len(test_accs) else 0.0\n",
    "\n",
    "    print(f\"\\n==================== {task_name.upper()} | {K_FOLDS}-FOLD SUMMARY ====================\")\n",
    "    for r in fold_results:\n",
    "        sz = r[\"sizes\"]\n",
    "        print(f\"fold{r['fold']} | Val(best) {r['best_val']:.4f} | Test {r['test_acc']:.4f} | \"\n",
    "              f\"train/val/test={sz['train']}/{sz['val']}/{sz['test']}\")\n",
    "    print(f\"MEANÂ±STD TestAcc = {mean_acc:.4f} Â± {std_acc:.4f}\")\n",
    "\n",
    "    return fold_results\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "all_results = []\n",
    "if RUN_ACTION:\n",
    "    all_results.extend(run_kfold(\"action\"))\n",
    "if RUN_USER:\n",
    "    all_results.extend(run_kfold(\"user\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0167c3",
   "metadata": {},
   "source": [
    "FASTHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastHAR need to be downloaded\n",
    "# -*- coding: utf-8 -*-  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pc_encoder.pointnet import pointnet_encoder\n",
    "from pc_encoder.pn2 import PointNet2Encoder\n",
    "from pc_encoder.pointmlp import PointMLPEncoder\n",
    "\n",
    "from pc_encoder.conv2d_dgcnn import DGCNN_encoder as conv2d_dgcnn_encoder\n",
    "from pc_encoder.conv2d_sa_dgcnn import conv2d_sa_dgcnn_encoder\n",
    "from pc_encoder.conv3d_dgcnn import conv3d_dgcnn_encoder\n",
    "from pc_encoder.conv3d_sa_dgcnn import conv3d_sa_dgcnn_encoder\n",
    "\n",
    "# Transformer\n",
    "from torch.nn import TransformerEncoder,TransformerEncoderLayer\n",
    "import math\n",
    "from timm.models.layers import trunc_normal_\n",
    "# mort sort\n",
    "from utils.mort_sort import simplied_morton_sorting,morton_sorting\n",
    "# rnn sort\n",
    "from utils.pointcloudRnn import PointCloudSortingRNN\n",
    "\n",
    "\n",
    "class pcseq_classifier(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(pcseq_classifier, self).__init__()\n",
    "        self.args = args\n",
    "        self.emb_dims = args.emb_dims\n",
    "        self.hidden_dims = args.hidden_dims\n",
    "        self.what_encoder = args.encoder\n",
    "        device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "        if self.what_encoder == 'conv2d_dgcnn':\n",
    "            self.encoder = conv2d_dgcnn_encoder(args).to(device) \n",
    "        elif self.what_encoder == 'conv2d_sa_dgcnn':\n",
    "            self.encoder = conv2d_sa_dgcnn_encoder(args).to(device)\n",
    "        elif self.what_encoder == 'conv3d_dgcnn':\n",
    "            self.encoder = conv3d_dgcnn_encoder(args).to(device)\n",
    "        elif self.what_encoder == 'conv3d_sa_dgcnn':\n",
    "            self.encoder = conv3d_sa_dgcnn_encoder(args).to(device)\n",
    "        elif self.what_encoder == 'pointnet':\n",
    "            self.encoder = pointnet_encoder(args).to(device)\n",
    "        elif self.what_encoder == 'pointnet2':\n",
    "            self.encoder = PointNet2Encoder(args).to(device)\n",
    "        elif self.what_encoder == 'pointmlp':\n",
    "            self.encoder = PointMLPEncoder(args,k_neighbors=[6],dim_expansion=[4],pre_blocks=[1],pos_blocks=[1],reducers=[2],normalize=\"center\",res_expansion=0.5).to(device)\n",
    "        \n",
    "        # transformer, wait to be optimized\n",
    "        transformer_encoder_layer = TransformerEncoderLayer(d_model=args.emb_dims, nhead=4, dim_feedforward=256, dropout=args.dropout, activation='relu')\n",
    "        self.transformer = TransformerEncoder(transformer_encoder_layer, num_layers=2)\n",
    "        # add cls token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.emb_dims))\n",
    "        self.cls_pos = nn.Parameter(torch.randn(1, 1, self.emb_dims))\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        trunc_normal_(self.cls_pos, std=.02)\n",
    "\n",
    "        #use transformer\n",
    "        self.fc1 = nn.Linear(args.hidden_dims*2 *2, args.hidden_dims*2) \n",
    "        self.fc1_5 = nn.Linear(args.hidden_dims*2, args.hidden_dims)\n",
    "        self.fc2 = nn.Linear(args.hidden_dims, int(args.hidden_dims/2))\n",
    "        self.bn1 = nn.BatchNorm1d(args.hidden_dims*2)\n",
    "        self.bn1_5 = nn.BatchNorm1d(args.hidden_dims)\n",
    "        self.bn2 = nn.BatchNorm1d(int(args.hidden_dims/2))\n",
    "        # self.bn1 = nn.BatchNorm1d(args.hidden_dims*2, affine=True)\n",
    "        # self.bn1_5 = nn.BatchNorm1d(args.hidden_dims, affine=True)\n",
    "        # self.bn2 = nn.BatchNorm1d(int(args.hidden_dims/2), affine=True)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.fc = nn.Linear(int(args.hidden_dims/2), args.num_classes)\n",
    "\n",
    "        # sort_method\n",
    "        self.sort = PointCloudSortingRNN(128, 256) if args.sort == 'rnn' else None\n",
    "        self.batch_sort = self.batch_pcseq_rnn if args.sort == 'rnn' else self.batch_pcseq_mort\n",
    "    \n",
    "    def generate_positional_encoding(self, seq_len, d_model):\n",
    "        PE = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        PE[:, 0::2] = torch.sin(position * div_term)\n",
    "        PE[:, 1::2] = torch.cos(position * div_term)\n",
    "        return PE\n",
    "        \n",
    "    def forward(self, x,frame_length):\n",
    "        x = self.batch_sort(x,frame_length)\n",
    "        x=x.permute(0,1,3,2)\n",
    "        batch_sz = x.size(0)\n",
    "\n",
    "        if '3d' not in self.what_encoder:\n",
    "            x = x.permute(1,0,2,3)\n",
    "            index_seq = np.argsort(frame_length.cpu().numpy()).tolist()\n",
    "            x = x[:,index_seq]\n",
    "            ascending_frame_length = sorted(frame_length)\n",
    "            reverse_to_original_index = np.argsort(index_seq)\n",
    "            y = torch.zeros((max(frame_length), batch_sz, self.emb_dims), device=x.device)\n",
    "            if self.what_encoder in ['conv2d_sa_dgcnn','conv2d_dgcnn','pointnet2','pointmlp','pointnet']:\n",
    "                head = 0\n",
    "                for m in range(max(frame_length)):\n",
    "                    while(m>=ascending_frame_length[head]):\n",
    "                        head += 1\n",
    "                    y[m][head:]= self.encoder(x[m][head:].type(torch.FloatTensor).to(x.device))\n",
    "                y = y[:,reverse_to_original_index]\n",
    "            y = y.permute(1,0,2)\n",
    "        else:\n",
    "            y = self.encoder(x) #y: (batch_size,frame_num,emb_dims)\n",
    "\n",
    "        # Transformer part\n",
    "        seq_len = y.size(1)  # Assuming y is of shape (batch_size, seq_len, d_model)\n",
    "        cls_tokens = self.cls_token.expand(batch_sz, -1, -1)\n",
    "        cls_pos = self.cls_pos\n",
    "        pos_encoding = self.generate_positional_encoding(seq_len, self.emb_dims).unsqueeze(0).to(y.device)\n",
    "        y = torch.cat((cls_tokens, y), dim=1)\n",
    "        pos_encoding = torch.cat((cls_pos, pos_encoding), dim=1)\n",
    "        y = y + pos_encoding\n",
    "        y = y.permute(1,0,2)\n",
    "        lengths = frame_length+1\n",
    "        src_key_padding_mask = (torch.arange(seq_len+1).to(x.device) >= lengths.unsqueeze(1)).to(x.device) #src_key_padding_mask: (batch_size,seq_len')\n",
    "        y = self.transformer(y,src_key_padding_mask=src_key_padding_mask) \n",
    "        y = y.permute(1,0,2) \n",
    "        # å–cls_tokenå¯¹åº”çš„è¾“å‡ºä½œä¸ºæ•´ä½“åºåˆ—çš„ç‰¹å¾è¡¨ç¤ºçš„ä¸€éƒ¨åˆ†\n",
    "        z1 = y[:,0,:].to(x.device)\n",
    "        # å–æœ€åŽä¸€ä¸ªæœ‰æ•ˆframeå¯¹åº”çš„è¾“å‡ºä½œä¸ºæ•´ä½“åºåˆ—çš„ç‰¹å¾è¡¨ç¤ºçš„ä¸€éƒ¨åˆ†/æœ‰æ•ˆframeä¸­æœ€å¤§å€¼\n",
    "        z2 = torch.zeros(batch_sz,self.emb_dims).to(x.device)\n",
    "        for i in range(batch_sz):\n",
    "            # z2[i] = y[i,frame_length[i],:]\n",
    "            valid_idx = min(frame_length[i].item(), y.shape[1] - 1)\n",
    "            z2[i] = y[i, valid_idx, :]\n",
    "\n",
    "        z = torch.cat((z1,z2),dim=1)\n",
    "\n",
    "        z = self.fc1(z) \n",
    "        y1 = self.bn1(z)  \n",
    "        y1 = F.leaky_relu(y1,negative_slope=0.2)\n",
    "        y1 = self.dropout(y1) \n",
    "\n",
    "        # add for use transformer\n",
    "        y1 = self.fc1_5(y1)\n",
    "        y1 = self.bn1_5(y1)\n",
    "        y1 = F.leaky_relu(y1,negative_slope=0.2)\n",
    "        y1 = self.dropout(y1)\n",
    "\n",
    "        y1 = self.fc2(y1)  \n",
    "        y1 = self.bn2(y1)   \n",
    "        y1 = F.leaky_relu(y1,negative_slope=0.2)\n",
    "        y1 = self.dropout(y1)\n",
    "\n",
    "        y1 = self.fc(y1)\n",
    "        return y1\n",
    "    \n",
    "\n",
    "    def batch_pcseq_mort(self,pcseq,frame_length):\n",
    "        b, f, n, c = pcseq.size()\n",
    "        for i in range(b):\n",
    "            #sorted_indices = simplied_morton_sorting(pcseq[i,:frame_length[i],:,:])\n",
    "            sorted_indices = morton_sorting(pcseq[i,:frame_length[i],:,:])\n",
    "            pcseq[i,:frame_length[i],:,:] = pcseq[i,:frame_length[i],:,:].view(-1,3)[sorted_indices,:].view(frame_length[i],n,c)\n",
    "        return pcseq\n",
    "    \n",
    "    def batch_pcseq_rnn(self,pcseq,frame_length):\n",
    "        b, f, n, c = pcseq.size()\n",
    "        pcseq = pcseq.view(-1,n,c)\n",
    "        id_base = torch.arange(0, b, device=pcseq.device) * f\n",
    "\n",
    "        indices = torch.cat([id_base[i] + torch.arange(frame_length[i], device=pcseq.device) for i in range(b)])\n",
    "\n",
    "        assert indices.size(0) == torch.sum(frame_length).item()\n",
    "\n",
    "        pcseq2 = torch.index_select(pcseq, 0, indices) # (indices.size(0),n,c)\n",
    "\n",
    "        sorted_indices = self.sort(pcseq2)  # (indices.size(0),n)\n",
    "\n",
    "        pcseq2 = pcseq2.gather(1, sorted_indices.unsqueeze(2).expand(-1, -1, pcseq2.size(2)))\n",
    "\n",
    "        pcseq.index_copy_(0, indices, pcseq2.view(-1, n, c))\n",
    "        pcseq = pcseq.view(b,f,n,c)\n",
    "        return pcseq\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Args:\n",
    "    exp_name = 'test_exp'\n",
    "    encoder = 'conv3d_sa_dgcnn'\n",
    "    dataset = 'pointcloud'\n",
    "    batch_size = 32\n",
    "    test_batch_size = 32\n",
    "    epochs = 500\n",
    "    use_sgd = False\n",
    "    lr = 0.001\n",
    "    momentum = 0.9\n",
    "    scheduler = 'cos'\n",
    "    no_cuda = False\n",
    "    seed = 1\n",
    "    eval = False\n",
    "    num_points = 256\n",
    "    dropout = 0.5\n",
    "    emb_dims = 256\n",
    "    hidden_dims = 128\n",
    "    k = 6\n",
    "    model_path = ''\n",
    "    num_classes = 21\n",
    "    dir = './data'  #\n",
    "    gpu = 0\n",
    "    depth = 2\n",
    "    sort = 'morton'\n",
    "    dataset_stride = 1\n",
    "    \n",
    "\n",
    "args = Args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "model = pcseq_classifier(args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b0239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, math, json, random\n",
    "import numpy as np\n",
    "import h5py\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from classification_model import pcseq_classifier  \n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "H5_PATH   = \"../Panto_fixed.h5\"\n",
    "CKPT_DIR  = \"./ckpt_fasthar_panto_5fold\"\n",
    "MODEL_TAG = \"fasthar\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "RUN_ACTION = True\n",
    "RUN_USER   = True\n",
    "\n",
    "K_FOLDS   = 5\n",
    "FOLD_SEED = 42\n",
    "VAL_RATIO = 0.1\n",
    "VAL_SEED  = 123\n",
    "\n",
    "BATCH_SIZE  = 32\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY  = True\n",
    "\n",
    "EPOCHS    = 200\n",
    "LR        = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "USE_ONECYCLE = True\n",
    "MAX_LR    = 1e-3\n",
    "\n",
    "PATIENCE  = 12\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "USE_AMP = True\n",
    "\n",
    "USE_BALANCED_SUBSET = False\n",
    "PER_CLASS = 450\n",
    "BALANCE_SEED = 12\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "def load_h5_dataset_both(filename):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        X = np.array(f[\"data\"]).astype(np.float32)     # (B,T,N,3)\n",
    "        L = np.array(f[\"labels\"]).astype(np.int64)     # (B,) or (B,2)=[action,user]\n",
    "    if L.ndim == 1:\n",
    "        action = L.copy()\n",
    "        user   = np.zeros_like(action)\n",
    "    else:\n",
    "        action = L[:, 0].copy()\n",
    "        user   = (L[:, 1].copy() if L.shape[1] > 1 else np.zeros(L.shape[0], dtype=np.int64))\n",
    "    return X, action, user\n",
    "\n",
    "def _normalize_global(seq_xyz, eps=1e-5):\n",
    "    pts = seq_xyz.reshape(-1, 3)\n",
    "    centroid = pts.mean(axis=0)\n",
    "    pts = pts - centroid\n",
    "    max_dist = np.linalg.norm(pts, axis=1).max()\n",
    "    scale = max(max_dist, eps)\n",
    "    return (pts / scale).reshape(seq_xyz.shape)\n",
    "\n",
    "def create_sequential_pointcloud_samples(X, normalize=True, per_frame=False, eps=1e-5, verbose=True):\n",
    "    assert X.ndim == 4 and X.shape[-1] == 3, f\"Expect (B,T,N,3), got {X.shape}\"\n",
    "    n = len(X)\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    out = np.empty_like(X, dtype=np.float32)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    if not normalize:\n",
    "        out[...] = X\n",
    "    else:\n",
    "        if per_frame:\n",
    "            for i in range(n):\n",
    "                T = X[i].shape[0]\n",
    "                tmp = np.empty_like(X[i], dtype=np.float32)\n",
    "                for t in range(T):\n",
    "                    fr = X[i, t]\n",
    "                    c = fr.mean(axis=0)\n",
    "                    fr = fr - c\n",
    "                    md = np.linalg.norm(fr, axis=1).max()\n",
    "                    tmp[t] = fr / max(md, eps)\n",
    "                out[i] = tmp\n",
    "        else:\n",
    "            for i in range(n):\n",
    "                out[i] = _normalize_global(X[i], eps=eps)\n",
    "    total = time.perf_counter() - t0\n",
    "    if verbose:\n",
    "        print(f\"[Normalize] {n} samples | total={total:.3f}s | avg={total/max(n,1)*1000:.3f} ms/sample\")\n",
    "    return out\n",
    "\n",
    "def remap_labels(y_raw):\n",
    "    y_raw = np.asarray(y_raw, dtype=np.int64)\n",
    "    uniq = np.unique(y_raw)\n",
    "    lut = {v: i for i, v in enumerate(uniq)}\n",
    "    y = np.array([lut[v] for v in y_raw], dtype=np.int64)\n",
    "    return y, int(len(uniq))\n",
    "\n",
    "def make_stratified_folds(pos_indices, y, k=5, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_indices = np.asarray(pos_indices, dtype=np.int64)\n",
    "    y_pool = y[pos_indices]\n",
    "\n",
    "    by_class = {}\n",
    "    for c in np.unique(y_pool):\n",
    "        idx_c = pos_indices[y_pool == c].copy()\n",
    "        rng.shuffle(idx_c)\n",
    "        by_class[int(c)] = idx_c\n",
    "\n",
    "    folds = [[] for _ in range(k)]\n",
    "    for c, idx_c in by_class.items():\n",
    "        parts = np.array_split(idx_c, k)\n",
    "        for f in range(k):\n",
    "            folds[f].extend(parts[f].tolist())\n",
    "\n",
    "    folds = [np.array(folds[f], dtype=np.int64) for f in range(k)]\n",
    "    for f in range(k):\n",
    "        rng.shuffle(folds[f])\n",
    "    return folds\n",
    "\n",
    "def stratified_train_val_split(pos_indices, y, val_ratio=0.1, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_indices = np.asarray(pos_indices, dtype=np.int64)\n",
    "    y_pool = y[pos_indices]\n",
    "    train, val = [], []\n",
    "    for c in np.unique(y_pool):\n",
    "        idx_c = pos_indices[y_pool == c].copy()\n",
    "        rng.shuffle(idx_c)\n",
    "        n_val = max(1, int(len(idx_c) * val_ratio))\n",
    "        val.extend(idx_c[:n_val].tolist())\n",
    "        train.extend(idx_c[n_val:].tolist())\n",
    "    rng.shuffle(train); rng.shuffle(val)\n",
    "    return np.array(train, dtype=np.int64), np.array(val, dtype=np.int64)\n",
    "\n",
    "def build_balanced_pool_indices(labels_raw, per_class=450, seed=12):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    by_c = defaultdict(list)\n",
    "    for i, y in enumerate(labels_raw):\n",
    "        by_c[int(y)].append(i)\n",
    "    pool = []\n",
    "    for c in sorted(by_c.keys()):\n",
    "        idxs = by_c[c]\n",
    "        rng.shuffle(idxs)\n",
    "        pool.extend(idxs[:min(per_class, len(idxs))])\n",
    "    return np.array(sorted(pool), dtype=np.int64)\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y, idxs):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.idxs = np.asarray(idxs, dtype=np.int64)\n",
    "        self.T = int(X.shape[1])\n",
    "\n",
    "    def __len__(self): return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        j = int(self.idxs[i])\n",
    "        seq = torch.from_numpy(self.X[j])            \n",
    "        label = int(self.y[j])\n",
    "        flen = self.T                               \n",
    "        return seq, label, flen, j                  \n",
    "\n",
    "def collate_fixed(batch):\n",
    "    seqs, labels, flens, sample_ids = zip(*batch)\n",
    "    x = torch.stack(seqs, dim=0)                                # (B,T,N,3)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    fl = torch.tensor(flens, dtype=torch.long)\n",
    "    sid = torch.tensor(sample_ids, dtype=torch.long)\n",
    "    return x, y, fl, sid\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "    ok, tot = 0, 0\n",
    "    for x, y, fl, _sid in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        fl = fl.to(device, non_blocking=True)\n",
    "        logits = model(x, fl)\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.numel()\n",
    "    return ok / max(tot, 1)\n",
    "\n",
    "def train_one_fold(model, train_loader, val_loader):\n",
    "    model.to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    if USE_ONECYCLE:\n",
    "        sch = optim.lr_scheduler.OneCycleLR(\n",
    "            opt, max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=max(1, len(train_loader))\n",
    "        )\n",
    "    else:\n",
    "        sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1, EPOCHS))\n",
    "\n",
    "    use_amp = bool(USE_AMP and device.type == \"cuda\")\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    best_val = -1.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        tr_ok, tr_tot, tr_loss = 0, 0, 0.0\n",
    "\n",
    "        for x, y, fl, _sid in train_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            fl = fl.to(device, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.autocast(device_type=\"cuda\", enabled=use_amp, dtype=torch.float16):\n",
    "                logits = model(x, fl)\n",
    "                loss = crit(logits, y)\n",
    "\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            if USE_ONECYCLE:\n",
    "                sch.step()\n",
    "\n",
    "            tr_loss += float(loss.item())\n",
    "            tr_ok += (logits.argmax(1) == y).sum().item()\n",
    "            tr_tot += y.numel()\n",
    "\n",
    "        if not USE_ONECYCLE:\n",
    "            sch.step()\n",
    "\n",
    "        tr_acc = tr_ok / max(tr_tot, 1)\n",
    "        va_acc = eval_acc(model, val_loader)\n",
    "        print(f\"Epoch {epoch:03d}/{EPOCHS} | loss={tr_loss:.4f} | tr={tr_acc:.4f} | va={va_acc:.4f}\")\n",
    "\n",
    "        if (va_acc - best_val) > MIN_DELTA:\n",
    "            best_val = va_acc\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= PATIENCE:\n",
    "                print(f\"â¹ï¸ Early stop (patience={PATIENCE})\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return float(best_val)\n",
    "\n",
    "def run_kfold(task_name, X_proc, action_raw, user_raw):\n",
    "    assert task_name in [\"action\", \"user\"]\n",
    "\n",
    "    labels_raw = action_raw if task_name == \"action\" else user_raw\n",
    "\n",
    "    if USE_BALANCED_SUBSET:\n",
    "        pool_indices = build_balanced_pool_indices(labels_raw, per_class=PER_CLASS, seed=BALANCE_SEED)\n",
    "        print(f\"[{task_name}] BALANCED pool={len(pool_indices)} (PER_CLASS={PER_CLASS})\")\n",
    "    else:\n",
    "        pool_indices = np.arange(len(labels_raw), dtype=np.int64)\n",
    "        print(f\"[{task_name}] pool={len(pool_indices)} (all)\")\n",
    "\n",
    "    y_pool_raw = labels_raw[pool_indices]\n",
    "    y_pool, num_classes = remap_labels(y_pool_raw)\n",
    "\n",
    "    all_pos = np.arange(len(pool_indices), dtype=np.int64)\n",
    "    folds = make_stratified_folds(all_pos, y_pool, k=K_FOLDS, seed=FOLD_SEED)\n",
    "\n",
    "    print(f\"\\n[{task_name.upper()}] classes={num_classes} | K={K_FOLDS} | VAL_RATIO={VAL_RATIO}\")\n",
    "\n",
    "    fold_results = []\n",
    "    for fold in range(K_FOLDS):\n",
    "        test_pos = folds[fold]\n",
    "        trainval_pos = np.setdiff1d(all_pos, test_pos, assume_unique=False)\n",
    "        train_pos, val_pos = stratified_train_val_split(trainval_pos, y_pool, val_ratio=VAL_RATIO, seed=VAL_SEED + fold)\n",
    "\n",
    "        # global indices\n",
    "        train_idx = pool_indices[train_pos]\n",
    "        val_idx   = pool_indices[val_pos]\n",
    "        test_idx  = pool_indices[test_pos]\n",
    "\n",
    "        # labels (remapped, aligned with pool positions)\n",
    "        y_train = y_pool[train_pos]\n",
    "        y_val   = y_pool[val_pos]\n",
    "        y_test  = y_pool[test_pos]\n",
    "\n",
    "        ds_tr = SeqDataset(X_proc, y_pool, train_idx) \n",
    "        ds_va = SeqDataset(X_proc, y_pool, val_idx)\n",
    "        ds_te = SeqDataset(X_proc, y_pool, test_idx)\n",
    "\n",
    "        y_global = np.full((len(X_proc),), -1, dtype=np.int64)\n",
    "        y_global[pool_indices] = y_pool\n",
    "        ds_tr = SeqDataset(X_proc, y_global, train_idx)\n",
    "        ds_va = SeqDataset(X_proc, y_global, val_idx)\n",
    "        ds_te = SeqDataset(X_proc, y_global, test_idx)\n",
    "\n",
    "        loader_kw = dict(\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            persistent_workers=(NUM_WORKERS > 0),\n",
    "            collate_fn=collate_fixed\n",
    "        )\n",
    "        tr_loader = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True,  **loader_kw)\n",
    "        va_loader = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, **loader_kw)\n",
    "        te_loader = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, **loader_kw)\n",
    "\n",
    "        args_task = deepcopy(args)\n",
    "        args_task.num_classes = int(num_classes)\n",
    "        model = pcseq_classifier(args_task).to(device)\n",
    "\n",
    "        print(f\"\\n----- {task_name.upper()} | FOLD {fold}/{K_FOLDS-1} -----\")\n",
    "        print(f\"[SPLIT] train/val/test = {len(ds_tr)}/{len(ds_va)}/{len(ds_te)}\")\n",
    "\n",
    "        best_val = train_one_fold(model, tr_loader, va_loader)\n",
    "        test_acc = eval_acc(model, te_loader)\n",
    "\n",
    "        ckpt_path = os.path.join(CKPT_DIR, f\"best_model_{MODEL_TAG}_{task_name}_fold{fold}.pth\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"ðŸ’¾ Saved: {ckpt_path}\")\n",
    "        print(f\"ðŸ”¥ Fold {fold} | BestVal={best_val:.4f} | TestAcc={test_acc:.4f}\")\n",
    "\n",
    "        fold_results.append({\n",
    "            \"task\": task_name,\n",
    "            \"fold\": int(fold),\n",
    "            \"num_classes\": int(num_classes),\n",
    "            \"best_val\": float(best_val),\n",
    "            \"test_acc\": float(test_acc),\n",
    "            \"sizes\": {\"train\": int(len(ds_tr)), \"val\": int(len(ds_va)), \"test\": int(len(ds_te))}\n",
    "        })\n",
    "\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    test_accs = [r[\"test_acc\"] for r in fold_results]\n",
    "    print(f\"\\n==== {task_name.upper()} SUMMARY ====\")\n",
    "    print(f\"MEANÂ±STD TestAcc = {float(np.mean(test_accs)):.4f} Â± {float(np.std(test_accs)):.4f}\")\n",
    "\n",
    "\n",
    "    out_json = os.path.join(CKPT_DIR, f\"{MODEL_TAG}_{task_name}_5fold_results.json\")\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(fold_results, f, indent=2)\n",
    "    print(f\"ðŸ“ Saved: {out_json}\")\n",
    "\n",
    "    return fold_results\n",
    "\n",
    "# -------------------------\n",
    "# RUN\n",
    "# -------------------------\n",
    "X, y_action_raw, y_user_raw = load_h5_dataset_both(H5_PATH)\n",
    "X_proc = create_sequential_pointcloud_samples(X, normalize=True, per_frame=False, verbose=True)\n",
    "\n",
    "all_results = []\n",
    "if RUN_ACTION:\n",
    "    all_results.extend(run_kfold(\"action\", X_proc, y_action_raw, y_user_raw))\n",
    "if RUN_USER:\n",
    "    all_results.extend(run_kfold(\"user\", X_proc, y_action_raw, y_user_raw))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cca8bb",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
